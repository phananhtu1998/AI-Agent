import inspect
import os
from uuid import uuid4
from typing import Optional, Dict, List
import asyncio

import httpx  # type: ignore[import-not-found]
from a2a.server.agent_execution import (  # type: ignore[import-not-found]
    AgentExecutor,
    RequestContext,
)
from a2a.server.events import EventQueue  # type: ignore[import-not-found]
from a2a.types import (  # type: ignore[import-not-found]
    Message,
    Role,
    TaskState,
    TaskStatus,
    TaskStatusUpdateEvent,
    TextPart,
)
import re
import unicodedata

try:
    # Optional: load .env if present
    from dotenv import load_dotenv  # type: ignore[import-not-found]
    load_dotenv()
except Exception:
    pass

try:
    # LangChain memory for conversation management
    from langchain.memory import ConversationBufferWindowMemory
    from langchain.schema import HumanMessage, AIMessage
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("LangChain not available, using fallback memory system")

_gemini_model = None

class FallbackMemory:
    """Fallback memory system khi kh√¥ng c√≥ LangChain."""
    
    def __init__(self, k: int = 5):
        self.k = k
        self.messages: List[Dict] = []
    
    def save_context(self, inputs: Dict, outputs: Dict):
        """L∆∞u context."""
        if 'input' in inputs:
            self.messages.append({
                'role': 'user',
                'content': inputs['input'],
                'timestamp': asyncio.get_event_loop().time()
            })
        
        if 'output' in outputs:
            self.messages.append({
                'role': 'assistant', 
                'content': outputs['output'],
                'timestamp': asyncio.get_event_loop().time()
            })
        
        # Gi·ªØ ch·ªâ k tin nh·∫Øn g·∫ßn nh·∫•t
        if len(self.messages) > self.k * 2:
            self.messages = self.messages[-self.k * 2:]
    
    def load_memory_variables(self, inputs: Dict) -> Dict:
        """Load memory variables."""
        if not self.messages:
            return {'history': ''}
        
        history_lines = []
        for msg in self.messages[-self.k * 2:]:
            role = "Ng∆∞·ªùi d√πng" if msg['role'] == 'user' else "Assistant"
            history_lines.append(f"{role}: {msg['content']}")
        
        return {'history': '\n'.join(history_lines)}
    
    def clear(self):
        """X√≥a memory."""
        self.messages.clear()

# Global memory store
_memory_store: Dict[str, any] = {}

def get_or_create_memory(context_id: str):
    """L·∫•y ho·∫∑c t·∫°o memory system."""
    if context_id not in _memory_store:
        if LANGCHAIN_AVAILABLE:
            # S·ª≠ d·ª•ng LangChain memory
            _memory_store[context_id] = ConversationBufferWindowMemory(
                k=5,  # Gi·ªØ 5 c·∫∑p Q&A
                return_messages=True,
                memory_key="history"
            )
        else:
            # Fallback memory
            _memory_store[context_id] = FallbackMemory(k=5)
    
    return _memory_store[context_id]


class GeminiAgentExecutor(AgentExecutor):
    """Gemini-backed Agent.

    Uses Google Gemini (e.g., gemini-2.5-flash) to generate responses based on
    the user's input extracted from the RequestContext.
    """

    async def invoke(self) -> str:
        # Fallback text if invoked without context (should not happen in server flow)
        return 'Hello from Gemini Agent'

    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        global _gemini_model

        # Extract user text input
        try:
            user_text = context.get_user_input() if hasattr(context, 'get_user_input') else ''
        except Exception:
            user_text = ''
        if not user_text:
            user_text = 'Hello'

        # L·∫•y context cho phi√™n n√†y
        context_id = getattr(context, 'context_id', 'default')
        memory = get_or_create_memory(context_id)
        
        # Kh√¥ng l∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng n·ªØa v√¨ ƒë√£ l∆∞u trong router
        # Ch·ªâ c·∫ßn l∆∞u ph·∫£n h·ªìi c·ªßa bot sau khi x·ª≠ l√Ω

        # Configure Gemini model lazily
        if _gemini_model is None:
            try:
                import google.generativeai as genai  # type: ignore[import-not-found]

                api_key = os.environ.get('GOOGLE_API_KEY') or os.environ.get('GEMINI_API_KEY')
                if not api_key:
                    raise RuntimeError('Missing GOOGLE_API_KEY (or GEMINI_API_KEY)')
                genai.configure(api_key=api_key)
                model_name = os.environ.get('GEMINI_MODEL', 'gemini-2.5-flash')
                _gemini_model = genai.GenerativeModel(model_name)
            except Exception as e:
                # On configuration error, emit a friendly message
                text = f"Gemini configuration error: {e}"
                msg = Message(
                    messageId=str(uuid4()),
                    role=Role.agent,
                    parts=[TextPart(text=text)],
                    taskId=getattr(context, 'task_id', None),
                    contextId=getattr(context, 'context_id', None),
                )
                maybe_awaitable = event_queue.enqueue_event(msg)
                if inspect.isawaitable(maybe_awaitable):
                    await maybe_awaitable
                return

        # Call Gemini v·ªõi context
        try:
            # T·∫°o prompt v·ªõi context
            history_text = memory.load_memory_variables({"input": user_text})['history']
            if history_text:
                enhanced_prompt = f"""Ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥:
{history_text}

C√¢u h·ªèi hi·ªán t·∫°i: {user_text}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh v√† c√¢u h·ªèi hi·ªán t·∫°i."""
            else:
                enhanced_prompt = user_text

            # Prefer async API if available
            response_text = None
            try:
                generate_content_async = getattr(_gemini_model, 'generate_content_async', None)
                if callable(generate_content_async):
                    resp = await generate_content_async(enhanced_prompt)
                    response_text = getattr(resp, 'text', None) if resp else None
                else:
                    # Fallback to sync call in a thread if async not available
                    import asyncio

                    loop = asyncio.get_running_loop()
                    resp = await loop.run_in_executor(None, _gemini_model.generate_content, enhanced_prompt)
                    response_text = getattr(resp, 'text', None) if resp else None
            except Exception as e:
                response_text = f"Gemini call failed: {e}"

            if not response_text:
                response_text = 'No response from Gemini.'

            # Th√™m ph·∫£n h·ªìi c·ªßa bot v√†o context
            memory.save_context({"input": user_text}, {"output": response_text})

            msg = Message(
                messageId=str(uuid4()),
                role=Role.agent,
                parts=[TextPart(text=response_text)],
                taskId=getattr(context, 'task_id', None),
                contextId=getattr(context, 'context_id', None),
            )
            maybe_awaitable = event_queue.enqueue_event(msg)
            if inspect.isawaitable(maybe_awaitable):
                await maybe_awaitable

        except Exception as e:
            # Emit a failure message
            msg = Message(
                messageId=str(uuid4()),
                role=Role.agent,
                parts=[TextPart(text=f'Error: {e}')],
                taskId=getattr(context, 'task_id', None),
                contextId=getattr(context, 'context_id', None),
            )
            maybe_awaitable = event_queue.enqueue_event(msg)
            if inspect.isawaitable(maybe_awaitable):
                await maybe_awaitable

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        # Emit a canceled status update per A2A contract
        task_id = getattr(context, 'task_id', None)
        context_id = getattr(context, 'context_id', None) or ''
        status = TaskStatus(state=TaskState.canceled)
        evt = TaskStatusUpdateEvent(
            taskId=task_id or '',
            contextId=context_id,
            status=status,
            final=True,
        )
        maybe_awaitable = event_queue.enqueue_event(evt)
        if inspect.isawaitable(maybe_awaitable):
            await maybe_awaitable


class WeatherAgentExecutor(AgentExecutor):
    """Simple weather agent using Open-Meteo geocoding and forecast APIs.

    Expects the user input to include a province/city. If missing or not
    found by geocoding, responds asking the user to provide the location.
    """

    GEOCODE_URL = 'https://geocoding-api.open-meteo.com/v1/search'
    WEATHER_URL = 'https://api.open-meteo.com/v1/forecast'
    
    # Class-level shared cache for Vietnam locations
    CLASS_LOCATIONS_CACHE: Dict[str, tuple] = {}
    CLASS_CACHE_TIMESTAMP: float = 0.0
    CLASS_CACHE_DURATION: float = 24 * 60 * 60  # 24 hours

    def __init__(self):
        super().__init__()
        # Instance-level references (mirror class cache)
        self._vietnam_locations_cache = None
        self._cache_timestamp = 0
        self._cache_duration = self.CLASS_CACHE_DURATION
        self._initialization_task = None
        
        # Fallback coordinates for major cities (backup)
        self.MAJOR_CITIES_FALLBACK = {
            'h√† n·ªôi': (21.0285, 105.8542, 'H√† N·ªôi'),
            'ha noi': (21.0285, 105.8542, 'H√† N·ªôi'),
            'h·ªì ch√≠ minh': (10.8231, 106.6297, 'H·ªì Ch√≠ Minh'),
            'ho chi minh': (10.8231, 106.6297, 'H·ªì Ch√≠ Minh'),
            'tp hcm': (10.8231, 106.6297, 'H·ªì Ch√≠ Minh'),
            'ƒë√† n·∫µng': (16.0544, 108.2022, 'ƒê√† N·∫µng'),
            'da nang': (16.0544, 108.2022, 'ƒê√† N·∫µng'),
            'h·∫£i ph√≤ng': (20.8449, 106.6881, 'H·∫£i Ph√≤ng'),
            'hai phong': (20.8449, 106.6881, 'H·∫£i Ph√≤ng'),
            'c·∫ßn th∆°': (10.0452, 105.7469, 'C·∫ßn Th∆°'),
            'can tho': (10.0452, 105.7469, 'C·∫ßn Th∆°'),
            'qu·∫£ng nam': (15.5394, 108.0191, 'Qu·∫£ng Nam'),
            'quang nam': (15.5394, 108.0191, 'Qu·∫£ng Nam'),
            'th·ª´a thi√™n hu·∫ø': (16.4637, 107.5909, 'Th·ª´a Thi√™n Hu·∫ø'),
            'thua thien hue': (16.4637, 107.5909, 'Th·ª´a Thi√™n Hu·∫ø'),
            'hu·∫ø': (16.4637, 107.5909, 'Th·ª´a Thi√™n Hu·∫ø'),
            'hue': (16.4637, 107.5909, 'Th·ª´a Thi√™n Hu·∫ø'),
            'kh√°nh h√≤a': (12.2388, 109.1967, 'Kh√°nh H√≤a'),
            'khanh hoa': (12.2388, 109.1967, 'Kh√°nh H√≤a'),
            'nha trang': (12.2388, 109.1967, 'Nha Trang'),
            'l√¢m ƒë·ªìng': (11.9404, 108.4583, 'L√¢m ƒê·ªìng'),
            'lam dong': (11.9404, 108.4583, 'L√¢m ƒê·ªìng'),
            'ƒë√† l·∫°t': (11.9404, 108.4583, 'ƒê√† L·∫°t'),
            'da lat': (11.9404, 108.4583, 'ƒê√† L·∫°t'),
            'b√¨nh d∆∞∆°ng': (11.1696, 106.6667, 'B√¨nh D∆∞∆°ng'),
            'binh duong': (11.1696, 106.6667, 'B√¨nh D∆∞∆°ng'),
            'ƒë·ªìng nai': (10.9574, 106.8426, 'ƒê·ªìng Nai'),
            'dong nai': (10.9574, 106.8426, 'ƒê·ªìng Nai'),
            'b√† r·ªãa v≈©ng t√†u': (10.5411, 107.2420, 'B√† R·ªãa V≈©ng T√†u'),
            'ba ria vung tau': (10.5411, 107.2420, 'B√† R·ªãa V≈©ng T√†u'),
            'v≈©ng t√†u': (10.3459, 107.0843, 'V≈©ng T√†u'),
            'vung tau': (10.3459, 107.0843, 'V≈©ng T√†u'),
        }
        
        # Start initialization in background only if class cache is empty
        if not type(self).CLASS_LOCATIONS_CACHE:
            self._start_initialization()

    def _start_initialization(self):
        """Start background initialization of Vietnam locations."""
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            self._initialization_task = loop.create_task(self._initialize_vietnam_locations())
            print("DEBUG: Started background initialization of Vietnam locations")
        except Exception as e:
            print(f"DEBUG: Failed to start initialization: {e}")

    async def _initialize_vietnam_locations(self):
        """Initialize Vietnam locations in background."""
        try:
            await self._fetch_vietnam_provinces()
            print("DEBUG: Background initialization completed")
        except Exception as e:
            print(f"DEBUG: Background initialization failed: {e}")

    async def _fetch_vietnam_provinces(self) -> Dict[str, tuple]:
        """L·∫•y danh s√°ch t·ªânh/th√†nh Vi·ªát Nam t·ª´ API."""
        print("DEBUG: Fetching Vietnam provinces from API...")
        
        try:
            # S·ª≠ d·ª•ng Open-Meteo geocoding API ƒë·ªÉ l·∫•y danh s√°ch t·ªânh/th√†nh
            async with httpx.AsyncClient(timeout=30) as client:
                # Query cho c√°c t·ªânh/th√†nh ch√≠nh c·ªßa Vi·ªát Nam
                vietnam_queries = [
                    "H√† N·ªôi", "H·ªì Ch√≠ Minh", "ƒê√† N·∫µng", "H·∫£i Ph√≤ng", "C·∫ßn Th∆°",
                    "Qu·∫£ng Nam", "Th·ª´a Thi√™n Hu·∫ø", "Kh√°nh H√≤a", "L√¢m ƒê·ªìng", 
                    "B√¨nh D∆∞∆°ng", "ƒê·ªìng Nai", "B√† R·ªãa V≈©ng T√†u", "V≈©ng T√†u",
                    "Ngh·ªá An", "Thanh H√≥a", "Qu·∫£ng Ninh", "B·∫Øc Ninh", "H·∫£i D∆∞∆°ng",
                    "Vƒ©nh Ph√∫c", "Th√°i Nguy√™n", "L√†o Cai", "Y√™n B√°i", "Tuy√™n Quang",
                    "Ph√∫ Th·ªç", "B·∫Øc Giang", "Qu·∫£ng B√¨nh", "Qu·∫£ng Tr·ªã", "Qu·∫£ng Ng√£i",
                    "B√¨nh ƒê·ªãnh", "Ph√∫ Y√™n", "B√¨nh Thu·∫≠n", "Ninh Thu·∫≠n", "B√¨nh Ph∆∞·ªõc",
                    "T√¢y Ninh", "Long An", "Ti·ªÅn Giang", "B·∫øn Tre", "Tr√† Vinh",
                    "Vƒ©nh Long", "ƒê·ªìng Th√°p", "An Giang", "Ki√™n Giang", "C√† Mau",
                    "B·∫°c Li√™u", "S√≥c TrƒÉng", "H·∫≠u Giang", "B√¨nh Ph∆∞·ªõc", "T√¢y Ninh"
                ]
                
                locations = {}
                
                for query in vietnam_queries:
                    try:
                        params = {
                            'name': query,
                            'count': 5,
                            'language': 'vi',
                            'format': 'json',
                        }
                        
                        r = await client.get(self.GEOCODE_URL, params=params)
                        if r.status_code == 200:
                            data = r.json()
                            results = data.get('results') or []
                            
                            # T√¨m k·∫øt qu·∫£ Vi·ªát Nam
                            vn_result = next((it for it in results if (it.get('country_code') or '').upper() == 'VN'), None)
                            if vn_result:
                                try:
                                    lat = float(vn_result.get('latitude'))
                                    lon = float(vn_result.get('longitude'))
                                    name = vn_result.get('name') or query
                                    
                                    # T·∫°o multiple keys cho t√™n ƒë·ªãa danh
                                    keys = [
                                        name.lower(),
                                        self._strip_diacritics(name).lower(),
                                        query.lower(),
                                        self._strip_diacritics(query).lower()
                                    ]
                                    
                                    for key in keys:
                                        if key and key not in locations:
                                            locations[key] = (lat, lon, name)
                                    
                                    print(f"DEBUG: Added {name} ({lat}, {lon})")
                                    
                                except (ValueError, TypeError) as e:
                                    print(f"DEBUG: Error parsing coordinates for {query}: {e}")
                                    continue
                        
                        # Th√™m delay nh·ªè ƒë·ªÉ tr√°nh rate limiting
                        await asyncio.sleep(0.1)
                        
                    except Exception as e:
                        print(f"DEBUG: Error fetching {query}: {e}")
                        continue
                
                # Merge v·ªõi fallback cities
                all_locations = {**self.MAJOR_CITIES_FALLBACK, **locations}
                
                # Update class-level cache and instance mirror
                now_ts = asyncio.get_event_loop().time()
                type(self).CLASS_LOCATIONS_CACHE = all_locations
                type(self).CLASS_CACHE_TIMESTAMP = now_ts
                self._vietnam_locations_cache = all_locations
                self._cache_timestamp = now_ts
                
                print(f"DEBUG: Successfully fetched {len(all_locations)} locations")
                return all_locations
                
        except Exception as e:
            print(f"DEBUG: Error fetching Vietnam provinces: {e}")
            # Fallback to static mapping
            now_ts = asyncio.get_event_loop().time()
            type(self).CLASS_LOCATIONS_CACHE = self.MAJOR_CITIES_FALLBACK
            type(self).CLASS_CACHE_TIMESTAMP = now_ts
            self._vietnam_locations_cache = self.MAJOR_CITIES_FALLBACK
            self._cache_timestamp = now_ts
            return self.MAJOR_CITIES_FALLBACK

    async def _get_vietnam_locations(self) -> Dict[str, tuple]:
        """L·∫•y danh s√°ch t·ªânh/th√†nh Vi·ªát Nam (cached)."""
        current_time = asyncio.get_event_loop().time()
        
        # Prefer class-level cache first
        if (type(self).CLASS_LOCATIONS_CACHE and
            current_time - type(self).CLASS_CACHE_TIMESTAMP < type(self).CLASS_CACHE_DURATION):
            print("DEBUG: Using cached Vietnam locations (class-level)")
            return type(self).CLASS_LOCATIONS_CACHE

        # Fallback to instance cache
        if (self._vietnam_locations_cache is not None and 
            current_time - self._cache_timestamp < self._cache_duration):
            print("DEBUG: Using cached Vietnam locations (instance-level)")
            return self._vietnam_locations_cache
        
        # N·∫øu cache expired ho·∫∑c ch∆∞a c√≥, fetch t·ª´ API
        print("DEBUG: Cache expired or not available, fetching from API...")
        return await self._fetch_vietnam_provinces()

    @classmethod
    async def preload_locations(cls) -> Dict[str, tuple]:
        """Preload Vietnam locations into class-level cache (call at app startup)."""
        tmp = cls()
        return await tmp._fetch_vietnam_provinces()

    async def _refresh_locations_cache(self):
        """Refresh cache manually."""
        print("DEBUG: Manually refreshing locations cache...")
        self._vietnam_locations_cache = None
        self._cache_timestamp = 0
        await self._fetch_vietnam_provinces()

    def _strip_diacritics(self, s: str) -> str:
        nfkd = unicodedata.normalize('NFKD', s)
        return ''.join(ch for ch in nfkd if not unicodedata.combining(ch))

    async def _extract_location(self, text: str) -> Optional[str]:
        original = (text or '').strip()
        if not original:
            return None
        lowered = original.lower()

        # Take text after the last weather keyword if present
        candidate = original
        for kw in ['th·ªùi ti·∫øt', 'thoi tiet', 'weather', 'nhi·ªát ƒë·ªô', 'nhiet do']:
            if kw in lowered:
                idx = lowered.rfind(kw)
                candidate = original[idx + len(kw):]
                break

        # Remove punctuation
        candidate = re.sub(r'[\.,!?;:]+', ' ', candidate)
        candidate = candidate.strip()

        # Remove common trailing time words
        candidate = re.sub(r'\b(h√¥m nay|hom nay|hi·ªán t·∫°i|hien tai)\b', '', candidate, flags=re.IGNORECASE).strip()

        print(f"DEBUG: Extracted location: '{candidate}' from '{original}'")
        return candidate or None

    async def _normalize_location_with_llm(self, location: str) -> str:
        """S·ª≠ d·ª•ng LLM ƒë·ªÉ chu·∫©n h√≥a t√™n ƒë·ªãa danh."""
        if not location.strip():
            return location
            
        global _gemini_model
        if _gemini_model is None:
            print(f"DEBUG: No Gemini model available, returning original: '{location}'")
            return location
            
        try:
            prompt = f"""
B·∫°n l√† chuy√™n gia v·ªÅ ƒë·ªãa danh Vi·ªát Nam. H√£y chu·∫©n h√≥a t√™n ƒë·ªãa danh sau:

Input: "{location}"

Y√™u c·∫ßu:
1. Tr·∫£ v·ªÅ t√™n ƒë·ªãa danh chu·∫©n, kh√¥ng c·∫ßn th√™m "t·ªânh", "th√†nh ph·ªë"
2. Gi·ªØ nguy√™n d·∫•u ti·∫øng Vi·ªát
3. N·∫øu l√† t√™n vi·∫øt t·∫Øt, vi·∫øt ƒë·∫ßy ƒë·ªß
4. Ch·ªâ tr·∫£ v·ªÅ t√™n ƒë·ªãa danh, kh√¥ng gi·∫£i th√≠ch

V√≠ d·ª•:
- "TP HCM" ‚Üí "H·ªì Ch√≠ Minh"
- "ƒê√† L·∫°t" ‚Üí "ƒê√† L·∫°t"
- "Qu·∫£ng Nam" ‚Üí "Qu·∫£ng Nam"
- "H√† N·ªôi" ‚Üí "H√† N·ªôi"

T√™n ƒë·ªãa danh chu·∫©n:
"""
            
            # Prefer async API if available
            response_text = None
            try:
                generate_content_async = getattr(_gemini_model, 'generate_content_async', None)
                if callable(generate_content_async):
                    resp = await generate_content_async(prompt)
                    response_text = getattr(resp, 'text', None) if resp else None
                else:
                    # Fallback to sync call in a thread if async not available
                    import asyncio
                    loop = asyncio.get_running_loop()
                    resp = await loop.run_in_executor(None, _gemini_model.generate_content, prompt)
                    response_text = getattr(resp, 'text', None) if resp else None
            except Exception as e:
                print(f"DEBUG: LLM normalization failed: {e}")
                return location

            if response_text:
                normalized = response_text.strip().strip('"').strip("'")
                print(f"DEBUG: LLM normalized '{location}' ‚Üí '{normalized}'")
                return normalized
            else:
                print(f"DEBUG: LLM returned empty response for '{location}'")
                
        except Exception as e:
            print(f"DEBUG: LLM normalization error: {e}")
            
        return location

    def _test_fallback_mapping(self):
        """Test fallback mapping functionality."""
        print("=== TESTING FALLBACK MAPPING ===")
        test_cases = ["Qu·∫£ng Nam", "QU·∫£ng Nam", "qu·∫£ng nam", "QUANG NAM"]
        
        # Use fallback cities for testing
        locations = self.MAJOR_CITIES_FALLBACK
        
        for test in test_cases:
            location_lower = test.lower().strip()
            print(f"Test: '{test}' ‚Üí '{location_lower}'")
            if location_lower in locations:
                lat, lon, name = locations[location_lower]
                print(f"  ‚úÖ Found: {name} at ({lat}, {lon})")
            else:
                print(f"  ‚ùå Not found")
                # Check if it's a substring match
                for key in locations.keys():
                    if location_lower in key or key in location_lower:
                        print(f"  üîç Similar key found: '{key}'")
        print("=== END TEST ===")

    async def _geocode(self, location: str) -> Optional[tuple[float, float, str]]:
        print(f"DEBUG: Geocoding location: '{location}'")
        
        # Test fallback mapping first
        self._test_fallback_mapping()
        
        # Chu·∫©n h√≥a t√™n ƒë·ªãa danh b·∫±ng LLM
        normalized_location = await self._normalize_location_with_llm(location)
        print(f"DEBUG: After LLM normalization: '{normalized_location}'")
        
        # L·∫•y danh s√°ch locations ƒë·ªông
        vietnam_locations = await self._get_vietnam_locations()
        
        # Check fallback mapping first
        location_lower = normalized_location.lower().strip()
        print(f"DEBUG: Checking dynamic locations for: '{location_lower}'")
        print(f"DEBUG: Available keys: {list(vietnam_locations.keys())[:10]}...")  # Show first 10 keys
        
        if location_lower in vietnam_locations:
            lat, lon, name = vietnam_locations[location_lower]
            print(f"DEBUG: Found in dynamic locations: {name} at ({lat}, {lon})")
            return (lat, lon, name)
        else:
            print(f"DEBUG: Not found in dynamic locations")
            # Try fuzzy matching
            for key in vietnam_locations.keys():
                if location_lower in key or key in location_lower:
                    lat, lon, name = vietnam_locations[key]
                    print(f"DEBUG: Fuzzy match found: '{key}' ‚Üí {name} at ({lat}, {lon})")
                    return (lat, lon, name)
        
        # Try multiple variants: original, normalized, without diacritics
        queries = [normalized_location]
        if normalized_location != location:
            queries.append(location)  # Keep original as backup
            
        no_diac = self._strip_diacritics(normalized_location)
        if no_diac and no_diac != normalized_location:
            queries.append(no_diac)
        
        print(f"DEBUG: Trying queries: {queries}")

        async with httpx.AsyncClient(timeout=10) as client:
            for q in queries:
                params = {
                    'name': q,
                    'count': 10,  # TƒÉng s·ªë l∆∞·ª£ng k·∫øt qu·∫£
                    'language': 'vi',
                    'format': 'json',
                }
                print(f"DEBUG: Querying with: {q}")
                r = await client.get(self.GEOCODE_URL, params=params)
                if r.status_code != 200:
                    print(f"DEBUG: HTTP {r.status_code} for query '{q}'")
                    continue
                data = r.json()
                results = data.get('results') or []
                print(f"DEBUG: Found {len(results)} results for '{q}'")
                
                if not results:
                    continue
                
                # Log all results for debugging
                for i, result in enumerate(results[:3]):  # Log first 3 results
                    print(f"DEBUG: Result {i+1}: {result.get('name')} ({result.get('country_code')})")
                
                # Prefer Vietnam results
                vn = next((it for it in results if (it.get('country_code') or '').upper() == 'VN'), None)
                item = vn or results[0]
                try:
                    coords = (
                        float(item.get('latitude')),
                        float(item.get('longitude')),
                        item.get('name') or normalized_location,
                    )
                    print(f"DEBUG: Selected: {coords[2]} at ({coords[0]}, {coords[1]})")
                    return coords
                except Exception as e:
                    print(f"DEBUG: Error parsing result: {e}")
                    continue
        
        print(f"DEBUG: No results found for '{location}'")
        return None

    async def _get_weather(self, lat: float, lon: float) -> Optional[str]:
        params = {
            'latitude': lat,
            'longitude': lon,
            'current': 'temperature_2m,weather_code',
        }
        async with httpx.AsyncClient(timeout=10) as client:
            r = await client.get(self.WEATHER_URL, params=params)
            if r.status_code != 200:
                return None
            j = r.json()
            cur = (j or {}).get('current') or {}
            temp = cur.get('temperature_2m')
            code = cur.get('weather_code')
            # Map a few common codes; otherwise show code number
            codes = {
                0: 'Tr·ªùi quang',
                1: 'Tr·ªùi quang ph·∫ßn l·ªõn',
                2: 'C√≥ m√¢y r·∫£i r√°c',
                3: 'Nhi·ªÅu m√¢y',
                45: 'S∆∞∆°ng m√π',
                51: 'M∆∞a ph√πn nh·∫π',
                61: 'M∆∞a nh·∫π',
                63: 'M∆∞a v·ª´a',
                65: 'M∆∞a to',
                71: 'Tuy·∫øt nh·∫π',
                80: 'M∆∞a r√†o nh·∫π',
                95: 'D√¥ng',
            }
            desc = codes.get(code, f'M√£ th·ªùi ti·∫øt {code}')
            if temp is None:
                return None
            return f'Nhi·ªát ƒë·ªô hi·ªán t·∫°i: {temp}¬∞C ‚Äî {desc}.'

    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        user_text = ''
        try:
            user_text = context.get_user_input() or ''
        except Exception:
            pass

        location_query = await self._extract_location(user_text)
        if not location_query or len(location_query.split()) < 1:
            reply = 'B·∫°n mu·ªën xem th·ªùi ti·∫øt ·ªü t·ªânh/th√†nh n√†o? Vui l√≤ng n√™u r√µ ƒë·ªãa danh.'
        else:
            loc = await self._geocode(location_query)
            if not loc:
                reply = 'Kh√¥ng t√¨m th·∫•y ƒë·ªãa danh. Vui l√≤ng cung c·∫•p t√™n t·ªânh/th√†nh c·ª• th·ªÉ.'
            else:
                lat, lon, place = loc
                weather = await self._get_weather(lat, lon)
                if weather:
                    reply = f'Th·ªùi ti·∫øt t·∫°i {place}: {weather}'
                else:
                    reply = 'Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu th·ªùi ti·∫øt. Vui l√≤ng th·ª≠ l·∫°i sau.'

        msg = Message(
            messageId=str(uuid4()),
            role=Role.agent,
            parts=[TextPart(text=reply)],
            taskId=getattr(context, 'task_id', None),
            contextId=getattr(context, 'context_id', None),
        )
        maybe_awaitable = event_queue.enqueue_event(msg)
        if inspect.isawaitable(maybe_awaitable):
            await maybe_awaitable

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        task_id = getattr(context, 'task_id', None)
        context_id = getattr(context, 'context_id', None) or ''
        status = TaskStatus(state=TaskState.canceled)
        evt = TaskStatusUpdateEvent(
            taskId=task_id or '',
            contextId=context_id,
            status=status,
            final=True,
        )
        maybe_awaitable = event_queue.enqueue_event(evt)
        if inspect.isawaitable(maybe_awaitable):
            await maybe_awaitable


class IntentRouterAgentExecutor(AgentExecutor):
    """Router s·ª≠ d·ª•ng intent classification thay v√¨ rule-based."""

    def __init__(self) -> None:
        self.weather = WeatherAgentExecutor()
        self.chat = GeminiAgentExecutor()
        self._intent_model = None  # C√≥ th·ªÉ cache Gemini ho·∫∑c model kh√°c
        # L∆∞u intent g·∫ßn nh·∫•t ƒë·ªÉ wrapper c√≥ th·ªÉ ƒë·ªçc
        self.last_intent: str | None = None

    async def _classify_intent(self, text: str, context_id: str = 'default') -> str:
        """Ph√¢n lo·∫°i intent b·∫±ng Gemini API v·ªõi context."""
        if not text.strip():
            return "chat"

        global _gemini_model
        if _gemini_model is None:
            try:
                import google.generativeai as genai
                api_key = os.environ.get("GOOGLE_API_KEY") or os.environ.get("GEMINI_API_KEY")
                if not api_key:
                    return "chat"
                genai.configure(api_key=api_key)
                _gemini_model = genai.GenerativeModel("gemini-2.5-flash")
            except Exception:
                return "chat"

        # L·∫•y memory context cho phi√™n n√†y
        memory = get_or_create_memory(context_id)
        history_text = memory.load_memory_variables({"input": text})['history']

        # T·∫°o prompt v·ªõi context
        if history_text:
            prompt = f"""
B·∫°n l√† b·ªô ph√¢n lo·∫°i intent th√¥ng minh.

Ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥:
{history_text}

C√¢u h·ªèi hi·ªán t·∫°i: "{text}"

Intent h·ª£p l·ªá: [weather, chat]

Quy t·∫Øc ph√¢n lo·∫°i:
- N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn th·ªùi ti·∫øt, nhi·ªát ƒë·ªô, kh√≠ h·∫≠u, d·ª± b√°o, m∆∞a, n·∫Øng, gi√≥, b√£o ‚Üí tr·∫£ "weather"
- N·∫øu c√¢u h·ªèi v·ªÅ ƒë·ªãa danh, t·ªânh, th√†nh ph·ªë m√† c√≥ th·ªÉ li√™n quan ƒë·∫øn th·ªùi ti·∫øt ‚Üí tr·∫£ "weather"
- N·∫øu c√¢u h·ªèi chung chung, kh√¥ng li√™n quan th·ªùi ti·∫øt ‚Üí tr·∫£ "chat"
- N·∫øu c√¢u h·ªèi ti·∫øp t·ª•c cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥ ‚Üí tr·∫£ "chat"

Ch·ªâ tr·∫£ ƒë√∫ng 1 t·ª´ (weather ho·∫∑c chat).
"""
        else:
            prompt = f"""
B·∫°n l√† b·ªô ph√¢n lo·∫°i intent. 
Ng∆∞·ªùi d√πng v·ª´a n√≥i: "{text}".
Intent h·ª£p l·ªá: [weather, chat].
N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn th·ªùi ti·∫øt, nhi·ªát ƒë·ªô, kh√≠ h·∫≠u, d·ª± b√°o ‚Üí tr·∫£ "weather".
N·∫øu kh√¥ng ‚Üí tr·∫£ "chat".
Ch·ªâ tr·∫£ ƒë√∫ng 1 t·ª´ (weather ho·∫∑c chat).
"""

        try:
            # Prefer async API if available
            response_text = None
            try:
                generate_content_async = getattr(_gemini_model, 'generate_content_async', None)
                if callable(generate_content_async):
                    resp = await generate_content_async(prompt)
                    response_text = getattr(resp, 'text', None) if resp else None
                else:
                    # Fallback to sync call in a thread if async not available
                    import asyncio
                    loop = asyncio.get_running_loop()
                    resp = await loop.run_in_executor(None, _gemini_model.generate_content, prompt)
                    response_text = getattr(resp, 'text', None) if resp else None
            except Exception:
                response_text = None

            if response_text:
                raw = response_text.strip().lower()
                if "weather" in raw:
                    return "weather"
            return "chat"
        except Exception:
            return "chat"

    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        text = ""
        try:
            text = context.get_user_input() or ""
        except Exception:
            pass

        # L·∫•y context_id cho phi√™n n√†y
        context_id = getattr(context, 'context_id', 'default')
        
        # L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng v√†o memory tr∆∞·ªõc khi ph√¢n lo·∫°i intent
        memory = get_or_create_memory(context_id)
        memory.save_context({"input": text}, {"output": ""})

        # Ph√¢n lo·∫°i intent v·ªõi context
        intent = await self._classify_intent(text, context_id)
        print(f"Intent: {intent} (Context ID: {context_id})")
        # Ghi l·∫°i intent ƒë·ªÉ b√™n ngo√†i c√≥ th·ªÉ ƒë·ªçc
        self.last_intent = intent
        
        if intent == "weather":
            await self.weather.execute(context, event_queue)
        else:
            await self.chat.execute(context, event_queue)

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        # Hu·ª∑ tr√™n c·∫£ 2 agent
        try:
            await self.weather.cancel(context, event_queue)
        except Exception:
            pass
        try:
            await self.chat.cancel(context, event_queue)
        except Exception:
            pass


# Backwards compatibility if imported elsewhere
HelloWorldAgentExecutor = GeminiAgentExecutor